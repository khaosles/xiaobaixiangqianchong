import httpx
import urllib.parse
from xml.etree import ElementTree as ET
from datetime import datetime
from typing import List, Optional
from base import BaseSearcher, PaperResult


class ArxivSearcher(BaseSearcher):
    """
    arXiv æ–‡çŒ®æœç´¢å™¨
    """
    
    def __init__(self):
        super().__init__("arXiv")
    
    async def search(self, query: str, max_results: int = 10, sorted: Optional[str] = None, **kwargs) -> List[PaperResult]:
        """
        ä½¿ç”¨ arXiv API æœç´¢è®ºæ–‡ï¼ˆå¼‚æ­¥æ–¹æ³•ï¼‰
        
        Args:
            query: æœç´¢å…³é”®è¯ï¼Œå¦‚ "large language models"
            max_results: æœ€å¤šè¿”å›å¤šå°‘ç¯‡
            sorted: æ’åºæ–¹å¼ï¼Œå¯é€‰å€¼ï¼š
                - "relevance" æˆ– None: æŒ‰ç›¸å…³æ€§æ’åºï¼ˆé»˜è®¤ï¼‰
                - "date" æˆ– "submittedDate": æŒ‰æäº¤æ—¥æœŸæ’åº
                - "lastUpdatedDate": æŒ‰æœ€åæ›´æ–°æ—¶é—´æ’åº
            **kwargs: å…¶ä»–å‚æ•°ï¼Œæ”¯æŒ sort_byï¼ˆå‘åå…¼å®¹ï¼Œä¼šè¢« sorted è¦†ç›–ï¼‰
            
        Returns:
            è®ºæ–‡ç»“æœåˆ—è¡¨
        """
        # å¤„ç†æ’åºå‚æ•°ï¼ˆæ”¯æŒ sorted å’Œå‘åå…¼å®¹çš„ sort_byï¼‰
        sort_by = kwargs.get('sort_by', None)
        if sorted is None:
            sorted = sort_by if sort_by else "relevance"
        
        # å°†ç®€åŒ–çš„æ’åºé€‰é¡¹è½¬æ¢ä¸º arXiv API æ ¼å¼
        # arXiv API æ”¯æŒçš„ sortBy å€¼: relevance, lastUpdatedDate, submittedDate
        if sorted == "date":
            sort_by_param = "submittedDate"
        elif sorted in ["relevance", None]:
            sort_by_param = "relevance"  # ä½¿ç”¨æ ‡å‡†çš„ relevanceï¼Œè€Œä¸æ˜¯ relevanceLastAuthorDate
        elif sorted == "lastUpdatedDate":
            sort_by_param = "lastUpdatedDate"
        else:
            # å¦‚æœæä¾›äº†å…¶ä»–å€¼ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨ï¼ˆå¯èƒ½æ˜¯æœ‰æ•ˆçš„ arXiv API å‚æ•°ï¼‰
            sort_by_param = sorted
        
        # arXiv API è¦æ±‚å¯¹æŸ¥è¯¢è¯ URL ç¼–ç 
        encoded_query = urllib.parse.quote(query)
        url = (
            f"https://export.arxiv.org/api/query?"
            f"search_query=all:{encoded_query}&"
            f"start=0&"
            f"max_results={max_results}&"
            f"sortBy={sort_by_param}&"
            f"sortOrder=descending"
        )

        try:
            async with httpx.AsyncClient(timeout=10.0, follow_redirects=True) as client:
                response = await client.get(url)
                response.raise_for_status()
                content = response.content
        except httpx.RequestError as e:
            print(f"arXiv è¯·æ±‚å¤±è´¥: {e}")
            return []
        except httpx.HTTPStatusError as e:
            print(f"arXiv HTTP é”™è¯¯: {e}")
            return []

        # è§£æ XML
        try:
            root = ET.fromstring(content)
        except ET.ParseError as e:
            print(f"arXiv XML è§£æå¤±è´¥: {e}")
            return []

        # å‘½åç©ºé—´
        ns = {
            'atom': 'http://www.w3.org/2005/Atom',
            'arxiv': 'http://arxiv.org/schemas/atom'
        }

        papers = []
        for entry in root.findall('atom:entry', ns):
            # æ ‡é¢˜ï¼ˆå»é™¤æ¢è¡Œï¼‰
            title_elem = entry.find('atom:title', ns)
            title = title_elem.text.strip().replace('\n', ' ') if title_elem is not None else "æ— æ ‡é¢˜"

            # æ‘˜è¦
            summary = entry.find('atom:summary', ns)
            abstract = summary.text.strip() if summary is not None else None

            # ä½œè€…
            authors = []
            for author in entry.findall('atom:author', ns):
                name = author.find('atom:name', ns)
                if name is not None:
                    authors.append(name.text)

            # å‘è¡¨æ—¥æœŸï¼ˆpublishedï¼‰
            published = entry.find('atom:published', ns)
            pub_date = None
            year = None
            if published is not None:
                try:
                    pub_date = datetime.strptime(published.text, "%Y-%m-%dT%H:%M:%SZ").strftime("%Y-%m-%d")
                    year = int(published.text[:4])
                except:
                    pub_date = published.text
                    year = self._extract_year(pub_date)

            # arXiv ID å’Œ PDF é“¾æ¥
            arxiv_id = None
            pdf_url = None
            abs_url = None
            for link in entry.findall('atom:link', ns):
                href = link.get('href')
                if href and 'arxiv.org/abs/' in href:
                    arxiv_id = href.split('/')[-1]
                    abs_url = href
                if link.get('title') == 'pdf':
                    pdf_url = href

            # åˆ†ç±»ï¼ˆprimary categoryï¼‰
            primary_cat = entry.find('arxiv:primary_category', ns)
            category = primary_cat.get('term') if primary_cat is not None else None

            # åˆ›å»ºç»“æœå¯¹è±¡
            paper = PaperResult(
                title=title,
                authors=self._normalize_authors(authors if authors else ['æœªçŸ¥ä½œè€…']),
                abstract=abstract,
                year=year,
                doi=None,  # arXiv é€šå¸¸æ²¡æœ‰ DOI
                url=abs_url,
                pdf_url=pdf_url,
                source=self.source_name,
                arxiv_id=arxiv_id,
                category=category,
                published_date=pub_date
            )
            
            papers.append(paper)

        return papers


# å‘åå…¼å®¹çš„å‡½æ•°æ¥å£ï¼ˆå¼‚æ­¥ï¼‰
async def search_arxiv(query: str, max_results: int = 10, sort_by: str = "relevance") -> List[dict]:
    """
    å‘åå…¼å®¹çš„å‡½æ•°æ¥å£ï¼ˆå¼‚æ­¥ï¼‰
    
    Args:
        query: æœç´¢å…³é”®è¯
        max_results: æœ€å¤šè¿”å›å¤šå°‘ç¯‡
        sort_by: æ’åºæ–¹å¼
        
    Returns:
        æ—§æ ¼å¼çš„å­—å…¸åˆ—è¡¨
    """
    searcher = ArxivSearcher()
    papers = await searcher.search(query, max_results, sorted=sort_by)
    
    # è½¬æ¢ä¸ºæ—§æ ¼å¼
    results = []
    for paper in papers:
        results.append({
            'title': paper.title,
            'authors': paper.authors,
            'abstract': paper.abstract,
            'published': paper.published_date,
            'arxiv_id': paper.arxiv_id,
            'category': paper.category,
            'pdf_url': paper.pdf_url,
            'abs_url': paper.url
        })
    
    return results


if __name__ == "__main__":
    import asyncio
    
    # ğŸ”§ æµ‹è¯•ç”¨ä¾‹ï¼ˆå¯ç›´æ¥è¿è¡Œï¼Œæ— éœ€å‘½ä»¤è¡Œå‚æ•°ï¼‰
    async def main():
        query = "resnet"  # â† åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„å…³é”®è¯
        limit = 1  # â† ä¿®æ”¹è¿”å›ç»“æœæ•°é‡

        print(f"æ­£åœ¨ arXiv æœç´¢: ã€Œ{query}ã€ (æœ€å¤š {limit} ç¯‡)...\n")

        searcher = ArxivSearcher()
        papers = await searcher.search(query, max_results=limit, sorted="date")

        if not papers:
            print("æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚")
        else:
            for i, p in enumerate(papers, 1):
                print(f"[{i}] {p.title}")
                print(f"   ä½œè€…: {', '.join(p.authors)}")
                print(f"   åˆ†ç±»: {p.category} | æ—¥æœŸ: {p.published_date}")
                print(f"   é¡µé¢: {p.url}")
                print(f"   PDF : {p.pdf_url}")
                print("   æ‘˜è¦:")
                if p.abstract:
                    print(f"   {p.abstract[:600]}{'...' if len(p.abstract) > 600 else ''}")
                else:
                    print("   ï¼ˆæ— æ‘˜è¦ï¼‰")
                print("-" * 80)
    
    asyncio.run(main())
